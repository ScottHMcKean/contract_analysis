{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Extract Metadata\n",
        "\n",
        "This notebook extracts detailed metadata from contracts for integration with contract management systems (like Icertis).\n",
        "\n",
        "**How it works:**\n",
        "1. You select a contract type (master_agreement, amendment, scope_of_work, or termination)\n",
        "2. The notebook loads a CSV file listing which files to process (or uses the classified table)\n",
        "3. It reads `metadata.csv` to determine which fields to extract for that contract type\n",
        "4. It loads a prompt template from `prompt_{contract_type}.md`\n",
        "5. It builds a dynamic SQL query that calls the LLM to extract all fields at once\n",
        "\n",
        "**Contract types supported:**\n",
        "- **master_agreement** -- full metadata extraction (parties, dates, terms, clauses, etc.)\n",
        "- **amendment** -- amendment-specific fields (parent agreement, what changed, new terms)\n",
        "- **scope_of_work** -- SOW-specific fields (deliverables, milestones, pricing, acceptance criteria)\n",
        "- **termination** -- termination-specific fields (reason, type, obligations, settlement)\n",
        "\n",
        "**Before you run this:**\n",
        "- Steps 1-4 must be complete\n",
        "- The `flat`, `doc_info`, and `classified` tables must exist\n",
        "- The `metadata.csv` and `prompt_{type}.md` files must be in the repo\n",
        "\n",
        "**Output tables:**\n",
        "- `metadata_{contract_type}` -- one table per contract type with all extracted fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Select the contract type from the dropdown. The notebook will load the right metadata fields and prompt template automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.widgets.text(\"catalog\", \"shm\", \"Catalog\")\n",
        "dbutils.widgets.text(\"schema\", \"contract\", \"Schema\")\n",
        "dbutils.widgets.dropdown(\"contract_type\", \"master_agreement\", [\"master_agreement\", \"amendment\", \"scope_of_work\", \"termination\"])\n",
        "dbutils.widgets.text(\"llm_endpoint\", \"databricks-claude-sonnet-4-5\", \"LLM Endpoint\")\n",
        "dbutils.widgets.text(\"batch_size\", \"100\", \"Batch Size\")\n",
        "dbutils.widgets.text(\"max_input_char\", \"400000\", \"Max Input Characters\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "contract_type = dbutils.widgets.get(\"contract_type\")\n",
        "llm_endpoint = dbutils.widgets.get(\"llm_endpoint\")\n",
        "batch_size = int(dbutils.widgets.get(\"batch_size\").strip())\n",
        "max_input_char = int(dbutils.widgets.get(\"max_input_char\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5A: Load File List\n",
        "\n",
        "If a CSV file exists for the selected contract type (e.g., `amendment_file_names.csv`), it will be used to filter which files to process. Otherwise, the classified table is used to find the right documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Try to load a file list CSV for the selected contract type\n",
        "file_list_path = f'./{contract_type}_file_names.csv'\n",
        "if os.path.exists(file_list_path):\n",
        "    file_list_df = pd.read_csv(file_list_path)\n",
        "    file_names = file_list_df['file_name'].tolist()\n",
        "    file_names_sql_tuple = \"(\" + \", \".join(f\"'{name}'\" for name in file_names) + \")\"\n",
        "    print(f\"Loaded {len(file_names)} files from {file_list_path}\")\n",
        "else:\n",
        "    file_names = []\n",
        "    file_names_sql_tuple = None\n",
        "    print(f\"No file list found at {file_list_path} -- will use classified table filter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5B: Build Metadata Prompt\n",
        "\n",
        "The `metadata.csv` file defines what fields to extract for each contract type. Each row has:\n",
        "- `type` -- which contract type this field belongs to\n",
        "- `metadata_name` -- the field name\n",
        "- `metadata_description` -- what to extract\n",
        "- `enum_fields` -- allowed values (if any)\n",
        "\n",
        "The prompt is assembled from the base template (`prompt_{type}.md`) plus the field definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata definitions and filter for the selected contract type\n",
        "metadata_df = pd.read_csv('./metadata.csv')\n",
        "metadata_df = metadata_df[metadata_df['type'] == contract_type]\n",
        "print(f\"Found {len(metadata_df)} metadata fields for {contract_type}\")\n",
        "\n",
        "column_names = ['path', 'contract_type'] + metadata_df['metadata_name'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the output table if it doesn't exist\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "table_schema = StructType([StructField(col, StringType(), True) for col in column_names])\n",
        "df = spark.createDataFrame([], table_schema)\n",
        "if not spark.catalog.tableExists(f\"{catalog}.{schema}.metadata_{contract_type}\"):\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.metadata_{contract_type}\")\n",
        "    print(f\"Created table: {catalog}.{schema}.metadata_{contract_type}\")\n",
        "else:\n",
        "    print(f\"Table already exists: {catalog}.{schema}.metadata_{contract_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the prompt from the template + field definitions\n",
        "with open(f\"./prompt_{contract_type}.md\", 'r') as f:\n",
        "    base_prompt = f.read().strip()\n",
        "\n",
        "fields = []\n",
        "response_struct_fields = []\n",
        "for _, row in metadata_df.iterrows():\n",
        "    name = row['metadata_name']\n",
        "    desc = row['metadata_description']\n",
        "    enum = row.get('enum_fields', None)\n",
        "    if pd.notnull(enum) and enum:\n",
        "        enum_str = f\" ENUM: {enum}\"\n",
        "    else:\n",
        "        enum_str = \"\"\n",
        "    fields.append(f\"- {name}: {desc}{enum_str}\")\n",
        "    response_struct_fields.append(f\"{name}:STRING\")\n",
        "\n",
        "prompt = base_prompt + \"\\n\\n\" + \"\\n\".join(fields)\n",
        "json_struct = \"STRUCT<\" + \",\".join(response_struct_fields) + \">\"\n",
        "response_format = f\"STRUCT<result:{json_struct}>\"\n",
        "\n",
        "print(f\"Prompt length: {len(prompt)} characters\")\n",
        "print(f\"Fields: {len(fields)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5C: Run Metadata Extraction\n",
        "\n",
        "This builds and executes the SQL query that calls the LLM for each contract. The query dynamically adapts based on the contract type:\n",
        "- **master_agreement** -- filters to classified master agreements, includes amendment info\n",
        "- **amendment** -- filters to files in the amendment file list, includes master agreement context\n",
        "- **scope_of_work** -- filters to SOW documents from doc_info classification\n",
        "- **termination** -- filters to termination documents from doc_info classification\n",
        "\n",
        "It skips documents already in the metadata table (idempotent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build WHERE clause and context fields based on contract type\n",
        "if contract_type == \"amendment\" and file_names_sql_tuple:\n",
        "    where_clause = f\"WHERE file_name in {file_names_sql_tuple}\"\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Related Master Agreement Name:', related_master_agreement_name, '\\\\n',\n",
        "          'Initial Master Agreement Expiry:', effective_date, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "elif contract_type == \"master_agreement\":\n",
        "    where_clause = \"WHERE c.is_master_agreement\"\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Has Amendments:', has_amendments, '\\\\n',\n",
        "          'Initial Master Agreement Expiry:', initial_master_agreement_expiry_date, '\\\\n',\n",
        "          'Final Master Agreement Expiry:', final_expiry_date, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "elif contract_type == \"scope_of_work\":\n",
        "    where_clause = \"WHERE d.document_type = 'SCOPE_OF_WORK' OR d.agreement_type = 'SOW'\"\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "elif contract_type == \"termination\":\n",
        "    where_clause = \"WHERE d.document_type = 'TERMINATION' OR d.agreement_type = 'TERMINATION'\"\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "else:\n",
        "    raise KeyError(f\"Unknown contract type: {contract_type}\")\n",
        "\n",
        "# Build the SQL query\n",
        "sql_query = f\"\"\"\n",
        "MERGE INTO {catalog}.{schema}.metadata_{contract_type} AS target\n",
        "USING (\n",
        "  SELECT\n",
        "    path,\n",
        "    '{contract_type}' as contract_type,\n",
        "    metadata.*,\n",
        "    cast(to_json(metadata) as string) AS combined_metadata\n",
        "  FROM (\n",
        "    SELECT \n",
        "      path,\n",
        "      from_json(AI_QUERY(\n",
        "        '{llm_endpoint}',\n",
        "        SUBSTRING(CONCAT(\n",
        "          '{prompt}', '\\\\n',\n",
        "          {context_fields}\n",
        "        ),0,{max_input_char}),\n",
        "        responseFormat => '{response_format}'\n",
        "      ), '{json_struct}'\n",
        "      ) as metadata\n",
        "    FROM (\n",
        "      SELECT * EXCEPT(c.path, d.path)\n",
        "      FROM {catalog}.{schema}.flat f\n",
        "      LEFT JOIN {catalog}.{schema}.doc_info d\n",
        "        ON f.path = d.path\n",
        "      LEFT JOIN {catalog}.{schema}.classified c\n",
        "        ON f.path = c.path\n",
        "      LEFT ANTI JOIN {catalog}.{schema}.metadata_{contract_type} m\n",
        "        ON f.path = m.path\n",
        "      {where_clause}\n",
        "      LIMIT CAST({batch_size} AS INT)\n",
        "    )\n",
        "  )\n",
        ") AS source\n",
        "ON target.path = source.path\n",
        "WHEN NOT MATCHED THEN INSERT *;\n",
        "\"\"\"\n",
        "\n",
        "print(\"Query built successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the metadata extraction query\n",
        "spark.sql(sql_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check results\n",
        "count = spark.sql(f\"SELECT * FROM {catalog}.{schema}.metadata_{contract_type}\").count()\n",
        "print(f\"There are {count} rows in metadata_{contract_type}\")\n",
        "spark.sql(f\"SELECT * FROM {catalog}.{schema}.metadata_{contract_type} LIMIT 5\").display()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
