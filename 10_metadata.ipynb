{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "# Contact Analysis\n",
        "## Metadata\n",
        "This notebook takes a CSV file (dropped into the workspace) and creats dynamic SQL queries for getting metadata out of the previous steps. The CSV is meant to make category management a lot easier - you only need metadata_name, metadata_description, and any ENUM categories and it will construct the SQL queries for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"schema\", \"\")\n",
        "dbutils.widgets.dropdown(\"contract_type\", \"master_agreement\", [\"master_agreement\", \"amendment\"])\n",
        "dbutils.widgets.text(\"llm_endpoint\", \"databricks-claude-sonnet-4-5\")\n",
        "dbutils.widgets.text(\"batch_size\", \"100\")\n",
        "dbutils.widgets.text(\"max_input_char\", \"400000\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "contract_type = dbutils.widgets.get(\"contract_type\")\n",
        "llm_endpoint = dbutils.widgets.get(\"llm_endpoint\")\n",
        "batch_size = int(dbutils.widgets.get(\"batch_size\").strip())\n",
        "max_input_char = int(dbutils.widgets.get(\"max_input_char\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load file list and filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load file list based on contract type\n",
        "if contract_type == \"amendment\":\n",
        "    file_list_df = pd.read_csv('./amendment_file_names.csv')\n",
        "    file_names = file_list_df['file_name'].tolist()\n",
        "else:\n",
        "    # For master agreements, we'll filter based on classified table\n",
        "    file_names = None\n",
        "\n",
        "# Create SQL tuple for filtering if we have file names\n",
        "if file_names:\n",
        "    file_names_sql_tuple = \"(\" + \", \".join(f\"'{name}'\" for name in file_names) + \")\"\n",
        "    print(f\"There are {len(file_names)} files in the list\")\n",
        "    print(file_names_sql_tuple)\n",
        "else:\n",
        "    file_names_sql_tuple = None\n",
        "    print(\"No file filter - will use classified table filter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create append-only contract file tracking table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create append-only tracking table for contract files\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.contract_files (\n",
        "  file_name STRING,\n",
        "  contract_type STRING,\n",
        "  added_timestamp TIMESTAMP\n",
        ")\n",
        "USING DELTA\n",
        "\"\"\")\n",
        "\n",
        "# Add files to tracking table if using file list and they don't already exist\n",
        "if file_names:\n",
        "    from pyspark.sql.functions import lit, current_timestamp\n",
        "    from pyspark.sql.types import StructType, StructField, StringType\n",
        "    \n",
        "    # Create dataframe with file names\n",
        "    file_df = spark.createDataFrame(\n",
        "        [(name, contract_type) for name in file_names],\n",
        "        [\"file_name\", \"contract_type\"]\n",
        "    ).withColumn(\"added_timestamp\", current_timestamp())\n",
        "    \n",
        "    # Only insert files that don't already exist\n",
        "    file_df.createOrReplaceTempView(\"new_files\")\n",
        "    spark.sql(f\"\"\"\n",
        "    MERGE INTO {catalog}.{schema}.contract_files AS target\n",
        "    USING new_files AS source\n",
        "    ON target.file_name = source.file_name\n",
        "    WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "SELECT * FROM IDENTIFIER(:catalog || '.' || :schema || '.contract_files')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make metadata prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata CSV\n",
        "metadata_df = pd.read_csv('./metadata.csv')\n",
        "\n",
        "# Filter metadata for the selected contract type\n",
        "metadata_df = metadata_df[metadata_df['type'] == contract_type]\n",
        "\n",
        "column_names = ['path', 'contract_type'] + metadata_df['metadata_name'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the metadata table if it doesn't exist\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "table_schema = StructType([StructField(col, StringType(), True) for col in column_names])\n",
        "df = spark.createDataFrame([], table_schema)\n",
        "if not spark.catalog.tableExists(f\"{catalog}.{schema}.metadata\"):\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.metadata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt from markdown file\n",
        "if contract_type == \"amendment\":\n",
        "    with open('./amendment_prompt.md', 'r') as f:\n",
        "        base_prompt = f.read().strip()\n",
        "else:\n",
        "    with open('./master_agreement_prompt.md', 'r') as f:\n",
        "        base_prompt = f.read().strip()\n",
        "\n",
        "# Build field list from metadata_df\n",
        "fields = []\n",
        "response_struct_fields = []\n",
        "for _, row in metadata_df.iterrows():\n",
        "    name = row['metadata_name']\n",
        "    desc = row['metadata_description']\n",
        "    enum = row.get('enum_fields', None)\n",
        "    if pd.notnull(enum) and enum:\n",
        "        enum_str = f\" ENUM: {enum}\"\n",
        "    else:\n",
        "        enum_str = \"\"\n",
        "    fields.append(f\"- {name}: {desc}{enum_str}\")\n",
        "    response_struct_fields.append(f\"{name}:STRING\")\n",
        "\n",
        "# Combine base prompt with fields at the bottom\n",
        "prompt = base_prompt + \"\\n\\n\" + \"\\n\".join(fields)\n",
        "\n",
        "json_struct = \"STRUCT<\" + \",\".join(response_struct_fields) + \">\"\n",
        "response_format = f\"STRUCT<result:{json_struct}>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure metadata table has all necessary columns\n",
        "fields_ddl = \",\\n  \".join([f\"{col} STRING\" for col in column_names])\n",
        "create_table_sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.metadata (\n",
        "  {fields_ddl}\n",
        ")\n",
        "USING DELTA\n",
        "\"\"\"\n",
        "spark.sql(create_table_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build WHERE clause based on contract type\n",
        "if contract_type == \"amendment\" and file_names_sql_tuple:\n",
        "    where_clause = f\"WHERE file_name in {file_names_sql_tuple}\"\n",
        "elif contract_type == \"master_agreement\":\n",
        "    where_clause = \"WHERE c.is_master_agreement\"\n",
        "else:\n",
        "    where_clause = \"\"\n",
        "\n",
        "# Build context fields based on contract type\n",
        "if contract_type == \"amendment\":\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Related Master Agreement Name:', related_master_agreement_name, '\\\\n',\n",
        "          'Initial Master Agreement Expiry:', effective_date, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "else:\n",
        "    context_fields = \"\"\"\n",
        "          'Vendor Name:', vendor_name, '\\\\n',\n",
        "          'File Path:', path, '\\\\n',\n",
        "          'Has Amendments:', has_amendments, '\\\\n',\n",
        "          'Initial Master Agreement Expiry:', initial_master_agreement_expiry_date, '\\\\n',\n",
        "          'Final Master Agreement Expiry:', final_expiry_date, '\\\\n',\n",
        "          'Doc Info:\\\\n', combined_doc_info, '\\\\n',\n",
        "          'Text:\\\\n', truncated, '\\\\n'\n",
        "    \"\"\"\n",
        "\n",
        "# Compose the SQL query\n",
        "sql_query = f\"\"\"\n",
        "MERGE INTO {catalog}.{schema}.metadata AS target\n",
        "USING (\n",
        "  SELECT\n",
        "    path,\n",
        "    '{contract_type}' as contract_type,\n",
        "    metadata.*,\n",
        "    cast(to_json(metadata) as string) AS combined_metadata\n",
        "  FROM (\n",
        "    SELECT \n",
        "      path,\n",
        "      from_json(AI_QUERY(\n",
        "        '{llm_endpoint}',\n",
        "        SUBSTRING(CONCAT(\n",
        "          '{prompt}', '\\\\n',\n",
        "          {context_fields}\n",
        "        ),0,{max_input_char}),\n",
        "        responseFormat => '{response_format}'\n",
        "      ), '{json_struct}'\n",
        "      ) as metadata\n",
        "    FROM (\n",
        "      SELECT * EXCEPT(c.path, d.path)\n",
        "      FROM {catalog}.{schema}.flat f\n",
        "      LEFT JOIN {catalog}.{schema}.doc_info d\n",
        "        ON f.path = d.path\n",
        "      LEFT JOIN {catalog}.{schema}.classified c\n",
        "        ON f.path = c.path\n",
        "      LEFT ANTI JOIN {catalog}.{schema}.metadata m\n",
        "        ON f.path = m.path\n",
        "      {where_clause}\n",
        "      LIMIT CAST({batch_size} AS INT)\n",
        "    )\n",
        "  )\n",
        ") AS source\n",
        "ON target.path = source.path\n",
        "WHEN NOT MATCHED THEN INSERT *;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the query!\n",
        "spark.sql(sql_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "SELECT * FROM IDENTIFIER(:catalog || '.' || :schema || '.metadata')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
