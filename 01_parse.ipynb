{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Parse Contracts\n",
        "\n",
        "This notebook handles the first stage of contract analysis: getting raw contract files into a usable format.\n",
        "\n",
        "**What it does:**\n",
        "1. Downloads contract PDFs from the Cook County open data portal\n",
        "2. Reads the raw files into a Delta table (so we can process them in parallel)\n",
        "3. Parses each document using Databricks AI (extracts text from PDFs, images, etc.)\n",
        "4. Flattens the parsed results into a clean table with full text and summaries\n",
        "\n",
        "**Before you run this:**\n",
        "- Set the widgets at the top (catalog, schema, volume name)\n",
        "- Make sure you have a Unity Catalog volume to store raw files\n",
        "- This step is the most time-consuming -- parsing large PDFs takes a while\n",
        "\n",
        "**Output tables:**\n",
        "- `bytes` -- raw file content\n",
        "- `parsed` -- AI-parsed document structure\n",
        "- `flat` -- cleaned text with preamble and truncated versions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your catalog, schema, and volume name using the widgets. These control where data is stored in Unity Catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.widgets.text(\"catalog\", \"shm\", \"Catalog\")\n",
        "dbutils.widgets.text(\"schema\", \"contract\", \"Schema\")\n",
        "dbutils.widgets.text(\"volume_name\", \"raw\", \"Volume Name\")\n",
        "dbutils.widgets.text(\"batch_size\", \"100\", \"Batch Size\")\n",
        "dbutils.widgets.text(\"words_preamble\", \"100\", \"Words in Preamble\")\n",
        "dbutils.widgets.text(\"words_truncated\", \"5000\", \"Words in Truncated Text\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "volume = dbutils.widgets.get(\"volume_name\")\n",
        "batch_size = int(dbutils.widgets.get(\"batch_size\"))\n",
        "doc_path = f\"/Volumes/{catalog}/{schema}/{volume}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1A: Download Contracts\n",
        "\n",
        "We use the Cook County of Illinois Procurement dataset: [Awarded Contracts & Amendments](https://catalog.data.gov/dataset/procurement-awarded-contracts-amendments).\n",
        "\n",
        "A local copy of the data is saved as `cook_county_contracts.parquet` in this repo. We filter to **Environmental Services** contracts to get a realistic set (~25 contracts with amendments).\n",
        "\n",
        "Each contract PDF is downloaded into a vendor-specific subfolder on your Unity Catalog volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "df = pd.read_parquet(\"cook_county_contracts.parquet\")\n",
        "env_df = df[df[\"Commodity Type\"] == \"Environmental Services\"]\n",
        "print(f\"Found {len(env_df)} Environmental Services rows\")\n",
        "display(env_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download each contract PDF into a vendor subfolder\n",
        "base_output_dir = doc_path\n",
        "Path(base_output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "downloaded = failed = 0\n",
        "\n",
        "for _, row in env_df[['Category', 'Vendor Name']].dropna(subset=['Category', 'Vendor Name']).iterrows():\n",
        "    category = row['Category']\n",
        "    vendor = re.sub(r'[^A-Z ]', '', str(row['Vendor Name']).upper())\n",
        "    match = re.search(r'(https?://[^\\s\\)]+)', str(category))\n",
        "    if match:\n",
        "        url = match.group(1)\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        vendor_dir = os.path.join(base_output_dir, vendor)\n",
        "        Path(vendor_dir).mkdir(parents=True, exist_ok=True)\n",
        "        output_path = os.path.join(vendor_dir, filename)\n",
        "        if os.system(f'wget -q -O \"{output_path}\" \"{url}\"') == 0:\n",
        "            downloaded += 1\n",
        "        else:\n",
        "            failed += 1\n",
        "\n",
        "print(f\"Done: {downloaded} files downloaded, {failed} failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1B: Read Files into Delta Table\n",
        "\n",
        "This reads all downloaded files into a `bytes` table. Each row stores:\n",
        "- The raw file content (as binary)\n",
        "- The vendor name and file name (extracted from the path)\n",
        "- A list of other file paths in the same vendor folder (useful later for linking amendments to master agreements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create the bytes table if it doesn't exist\n",
        "CREATE TABLE IF NOT EXISTS IDENTIFIER(:catalog || '.' || :schema || '.bytes') (\n",
        "  path STRING,\n",
        "  modificationTime TIMESTAMP,\n",
        "  length BIGINT,\n",
        "  _metadata STRUCT<\n",
        "    file_path: STRING, \n",
        "    file_name: STRING, \n",
        "    file_size: BIGINT, \n",
        "    file_block_start: BIGINT, \n",
        "    file_block_length: BIGINT, \n",
        "    file_modification_time: TIMESTAMP\n",
        "  >,\n",
        "  content BINARY,\n",
        "  vendor_name STRING,\n",
        "  file_name STRING,\n",
        "  vendor_folder_paths ARRAY<STRING>,\n",
        "  CONSTRAINT bytes_path_pk PRIMARY KEY (path)\n",
        ");\n",
        "\n",
        "-- Read raw files and merge into the bytes table\n",
        "MERGE INTO IDENTIFIER(:catalog || '.' || :schema || '.bytes') AS target\n",
        "USING (\n",
        "  WITH main_files AS (\n",
        "    SELECT\n",
        "      path,\n",
        "      modificationTime,\n",
        "      length,\n",
        "      _metadata,\n",
        "      content,\n",
        "      regexp_extract(path, :doc_path || '/([^/]+)/', 1) AS vendor_name,\n",
        "      regexp_extract(path, '/([^/]+)$', 1) AS file_name\n",
        "    FROM READ_FILES(:doc_path, format => 'binaryFile', recursiveFileLookup => true)\n",
        "  ),\n",
        "  all_vendor_files AS (\n",
        "    SELECT\n",
        "      path, \n",
        "      regexp_extract(path, :doc_path || '/([^/]+)/', 1) AS vendor_name,\n",
        "      regexp_extract(path, '/([^/]+)$', 1) AS file_name\n",
        "    FROM READ_FILES(:doc_path, format => 'binaryFile', recursiveFileLookup => true)\n",
        "  )\n",
        "  SELECT\n",
        "    m.*,\n",
        "    (\n",
        "      SELECT collect_list(avf.path)\n",
        "      FROM all_vendor_files avf\n",
        "      WHERE avf.vendor_name = m.vendor_name\n",
        "        AND avf.file_name != m.file_name\n",
        "    ) AS vendor_folder_paths\n",
        "  FROM main_files m\n",
        ") AS source\n",
        "ON target.path = source.path\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1C: Parse Documents with AI\n",
        "\n",
        "This uses Databricks `AI_PARSE_DOCUMENT` to extract structured text from each file. It processes files in batches to avoid timeouts.\n",
        "\n",
        "The loop keeps running until all files are parsed. If it fails partway through, just re-run -- it picks up where it left off.\n",
        "\n",
        "We start with PDFs and images, then do Office files separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "source": [
        "CREATE TABLE IF NOT EXISTS IDENTIFIER(:catalog || '.' || :schema || '.parsed') (\n",
        "  path STRING NOT NULL PRIMARY KEY,\n",
        "  parsed VARIANT\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Parse PDFs and images first\n",
        "file_pattern = r'\\.(pdf|jpg|jpeg|png)$'\n",
        "\n",
        "def remaining_count():\n",
        "    return spark.sql(f\"\"\"\n",
        "        SELECT COUNT(*) AS cnt\n",
        "        FROM {catalog}.{schema}.bytes AS b\n",
        "        LEFT JOIN {catalog}.{schema}.parsed AS p\n",
        "          ON b.path = p.path\n",
        "        WHERE b.file_name RLIKE '{file_pattern}'\n",
        "          AND p.path IS NULL\n",
        "    \"\"\").collect()[0][\"cnt\"]\n",
        "\n",
        "batch = 0\n",
        "start = time.time()\n",
        "\n",
        "while True:\n",
        "    remaining = remaining_count()\n",
        "    print(f\"Batch {batch+1}, remaining: {remaining}\")\n",
        "    if remaining == 0:\n",
        "        break\n",
        "\n",
        "    t0 = time.time()\n",
        "    spark.sql(f\"\"\"\n",
        "        MERGE INTO {catalog}.{schema}.parsed AS target\n",
        "        USING (\n",
        "          SELECT \n",
        "            b.path,\n",
        "            AI_PARSE_DOCUMENT(b.content) AS parsed\n",
        "          FROM (\n",
        "            SELECT b.path, content\n",
        "            FROM {catalog}.{schema}.bytes AS b\n",
        "            LEFT JOIN {catalog}.{schema}.parsed AS p\n",
        "              ON b.path = p.path\n",
        "            WHERE b.file_name RLIKE '{file_pattern}'\n",
        "              AND p.path IS NULL\n",
        "            ORDER BY b.length\n",
        "            LIMIT CAST({batch_size} AS INTEGER)\n",
        "          ) AS b\n",
        "        ) AS source\n",
        "        ON target.path = source.path\n",
        "        WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        "    print(f\"  Batch {batch+1} done in {time.time() - t0:.1f}s\")\n",
        "    batch += 1\n",
        "\n",
        "print(f\"All PDFs/images done in {time.time() - start:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now parse Office files (doc, docx, ppt, pptx)\n",
        "file_pattern = r'\\.(doc|docx|ppt|pptx)$'\n",
        "\n",
        "batch = 0\n",
        "start = time.time()\n",
        "\n",
        "while True:\n",
        "    remaining = remaining_count()\n",
        "    print(f\"Batch {batch+1}, remaining: {remaining}\")\n",
        "    if remaining == 0:\n",
        "        break\n",
        "\n",
        "    t0 = time.time()\n",
        "    spark.sql(f\"\"\"\n",
        "        MERGE INTO {catalog}.{schema}.parsed AS target\n",
        "        USING (\n",
        "          SELECT \n",
        "            b.path,\n",
        "            AI_PARSE_DOCUMENT(b.content) AS parsed\n",
        "          FROM (\n",
        "            SELECT path, content\n",
        "            FROM {catalog}.{schema}.bytes AS b\n",
        "            LEFT JOIN {catalog}.{schema}.parsed AS p\n",
        "              ON b.path = p.path\n",
        "            WHERE b.file_name RLIKE '{file_pattern}'\n",
        "              AND p.path IS NULL\n",
        "            ORDER BY b.length\n",
        "            LIMIT CAST({batch_size} AS INTEGER)\n",
        "          ) AS b\n",
        "        ) AS source\n",
        "        ON target.path = source.path\n",
        "        WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        "    print(f\"  Batch {batch+1} done in {time.time() - t0:.1f}s\")\n",
        "    batch += 1\n",
        "\n",
        "print(f\"All Office files done in {time.time() - start:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean up failed parses\n",
        "\n",
        "Sometimes parsing produces nearly empty results. This removes those so they get re-processed on the next run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "DELETE FROM IDENTIFIER(:catalog || '.' || :schema || '.parsed')\n",
        "WHERE length(concat_ws(\n",
        "        '\\n\\n',\n",
        "        transform(\n",
        "          try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
        "          element -> try_cast(element:content AS STRING)\n",
        "        )\n",
        "      )) < 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1D: Flatten Parsed Data\n",
        "\n",
        "This takes the structured parse output and creates a clean `flat` table with:\n",
        "- **text** -- the full extracted text of the document\n",
        "- **preamble** -- the first ~100 words (used for quick identification)\n",
        "- **truncated** -- the first ~5000 words (used as input to the LLM, since full text is often too long)\n",
        "\n",
        "The flat table also joins in vendor name, file name, and sibling file paths from the `bytes` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE TABLE IDENTIFIER(:catalog || '.' || :schema || '.flat') AS\n",
        "WITH flattened AS (\n",
        "  SELECT\n",
        "    * EXCEPT(b.path),\n",
        "    concat_ws(\n",
        "        '\\n\\n',\n",
        "        transform(\n",
        "          try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
        "          element -> try_cast(element:content AS STRING)\n",
        "        )\n",
        "      ) AS text,\n",
        "    concat_ws(\n",
        "        ' ',\n",
        "        slice(\n",
        "          split(\n",
        "            concat_ws(\n",
        "              '\\n\\n',\n",
        "              transform(\n",
        "                try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
        "                element -> try_cast(element:content AS STRING)\n",
        "              )\n",
        "            ),\n",
        "            ' '\n",
        "          ),\n",
        "          1,\n",
        "          :words_preamble\n",
        "        )\n",
        "      ) AS preamble,\n",
        "    concat_ws(\n",
        "        ' ',\n",
        "        slice(\n",
        "          split(\n",
        "            concat_ws(\n",
        "              '\\n\\n',\n",
        "              transform(\n",
        "                try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
        "                element -> try_cast(element:content AS STRING)\n",
        "              )\n",
        "            ),\n",
        "            ' '\n",
        "          ),\n",
        "          1,\n",
        "          :words_truncated\n",
        "        )\n",
        "      ) AS truncated\n",
        "  FROM IDENTIFIER(:catalog || '.' || :schema || '.parsed') p\n",
        "  LEFT JOIN (\n",
        "    SELECT * EXCEPT(content)\n",
        "    FROM IDENTIFIER(:catalog || '.' || :schema || '.bytes')\n",
        "  ) b\n",
        "  ON p.path = b.path\n",
        ")\n",
        "SELECT * FROM flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Quick check: how many documents do we have?\n",
        "SELECT COUNT(*) as total_docs FROM IDENTIFIER(:catalog || '.' || :schema || '.flat')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}